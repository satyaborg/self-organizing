# general
project: self-organizing
log_level: INFO
debug: 1

# global -- fixed arguments
seed: 42
device: cuda
n_classes: 10
early_stop: False
bins: 100 # 1000 is optimal (?)

# variable arguments
training_mode: handcrafted # options: handcrafted, meta_learning
ae_units: [5, 3] # a list of units
runs: 1

# datasets
# MNIST -- comment this if using CIFAR10
dataset: MNIST
hidden_units: 100 # default: 100
patch_size: 10 # default: 10 (receptive field size)
channels: 1

# # CIFAR10 -- comment this if using MNIST
# dataset: CIFAR10
# hidden_units: 256 # default: 256
# patch_size: 16 # default: 16
# channels: 1 # default: 1

# filepaths (creates folder if they don't exist)
paths:
  root: self-organizing/
  data: data/
  results: results/
  checkpoint: checkpoints/
  logs: logs/

# training hyperparams
hyperparams:
  meta_network:
    epochs: 2 # 100
    batch_size: 128 # 32
    valid_pct: 0.2
    dropout: 0.2
    num_workers: 4
    # scheduler: CosineAnnealingLR # optional
    lr: 0.01 # 0.001
    wd: 0.0001 # 1e-04
    t_max: 10 # for scheduler
  classifier:
    epochs: 1
    lr: 0.01 # 0.001
    wd: 0.0001 # 1e-04
    num_layers: 1
    
# meta:
#   heuristic: fchc, # options: fchc, rw, shc, sa
#   seed: 42,
#   dataset: mnist,
#   cuda: 1,
#   num_runs: 3,
#   early_stop: 1,
#   meta_steps: 20,
#   num_layers: 2,
#   units: [7],
#   local_entropy: 0,
#   cleared_weights: 0,
#   hill_climber: sa,
#   prob_function: standard,
#   schedule: exp,
#   T_initial: 8,
#   T_final: 0.01,
#   T_step: 0,
#   k: 0.25,
#   max_ptb: 10,
#   epochs_ae: 2,
#   epochs_logit: 2,
#   calc_accuracy: true   